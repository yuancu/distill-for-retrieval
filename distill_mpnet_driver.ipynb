{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Model Distillation Driver\n",
    "\n",
    "## Plan Overview\n",
    "\n",
    "**Teacher**: infly/inf-retriever-v1-pro (3584d, MRL-trained)\n",
    "\n",
    "**Student**: sentence-transformers/all-mpnet-base-v2 (768d)\n",
    "\n",
    "**Two Distillation Modes:**\n",
    "\n",
    "### Mode 1: With Projection (Higher Quality)\n",
    "- Architecture: Student (768d) + Projection (768→1536→3584)\n",
    "- Teacher target: Full 3584d embeddings\n",
    "- Best for: Maximum quality, when computational cost is acceptable\n",
    "\n",
    "### Mode 2: MRL-Based (More Efficient)\n",
    "- Architecture: Student (768d) only, no projection\n",
    "- Teacher target: First 768d of teacher embeddings (MRL slicing)\n",
    "- Best for: Efficiency, leverages teacher's Matryoshka training\n",
    "- Lower computational cost, smaller model size\n",
    "\n",
    "**Phase 1**: General Distillation\n",
    "- Loss: MSE (0.4) + Cosine (0.6)\n",
    "- Data: MS MARCO + NQ + HotpotQA\n",
    "\n",
    "**Phase 2**: Task-Specific\n",
    "- Loss: InfoNCE (0.8) + MSE (0.2)\n",
    "- Data: MS MARCO with hard negatives\n",
    "- Temperature: 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"4,5,6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import logging\n",
    "\n",
    "# Import from distill module\n",
    "from distill import (\n",
    "    ProjectionLayer,\n",
    "    StudentModelWithProjection,\n",
    "    train_phase1,\n",
    "    train_phase2,\n",
    "    evaluate_retrieval\n",
    ")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Available GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "TEACHER_MODEL = \"infly/inf-retriever-v1-pro\"\n",
    "STUDENT_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "TEACHER_DIM = 3584\n",
    "STUDENT_DIM = 768\n",
    "PROJECTION_HIDDEN_DIM = 1536\n",
    "\n",
    "# ============================================================================\n",
    "# DISTILLATION MODE CONFIGURATION\n",
    "# ============================================================================\n",
    "# Set USE_PROJECTION to choose between two distillation approaches:\n",
    "#\n",
    "# Option 1: WITH PROJECTION (use_projection=True)\n",
    "#   - Student (768d) -> Projection Layer -> 3584d output\n",
    "#   - Compared against full teacher embeddings (3584d)\n",
    "#   - Best for maximum quality, higher computational cost\n",
    "#\n",
    "# Option 2: MRL-BASED (use_projection=False)  \n",
    "#   - Student (768d) -> Direct output (no projection)\n",
    "#   - Compared against teacher's first 768d (MRL slicing)\n",
    "#   - Best for efficiency, lower computational cost\n",
    "#   - Leverages teacher's Matryoshka Representation Learning (MRL) training\n",
    "# ============================================================================\n",
    "USE_PROJECTION = False  # Set to False for MRL-based distillation\n",
    "\n",
    "# Phase 1 configuration\n",
    "PHASE1_CONFIG = {\n",
    "    'batch_size': 128,\n",
    "    'learning_rate': 2e-5,\n",
    "    'warmup_steps': 1000,\n",
    "    'num_epochs': 1,\n",
    "    'mse_weight': 0.4,\n",
    "    'cosine_weight': 0.6,\n",
    "    'max_length': 512,\n",
    "    'gradient_accumulation_steps': 4,\n",
    "    'max_samples_per_dataset': 10000\n",
    "}\n",
    "\n",
    "# Phase 2 configuration\n",
    "PHASE2_CONFIG = {\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 5e-6,\n",
    "    'warmup_steps': 500,\n",
    "    'num_epochs': 1,\n",
    "    'infonce_weight': 0.8,\n",
    "    'mse_weight': 0.2,\n",
    "    'temperature': 0.02,\n",
    "    'max_length': 512,\n",
    "    'num_negatives': 7,\n",
    "    'gradient_accumulation_steps': 8,\n",
    "    'max_samples': 100\n",
    "}\n",
    "\n",
    "# Paths\n",
    "OUTPUT_DIR = \"./checkpoints\"\n",
    "PHASE1_CHECKPOINT = os.path.join(OUTPUT_DIR, \"phase1_best\")\n",
    "PHASE2_CHECKPOINT = os.path.join(OUTPUT_DIR, \"phase2_best\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Multi-GPU Training with DDP\n",
    "\n",
    "**This notebook runs on a single GPU.** For faster training with multiple GPUs, use the DDP launcher script:\n",
    "\n",
    "### Quick Start - Multi-GPU Training\n",
    "\n",
    "```bash\n",
    "# Training with 8 GPUs (recommended)\n",
    "cd /home/yuanchu/src/distill\n",
    "torchrun --nproc_per_node=8 train_ddp.py\n",
    "\n",
    "# OR use the shell script\n",
    "./run_ddp.sh 8 True  # 8 GPUs, with projection\n",
    "./run_ddp.sh 8 False  # 8 GPUs, MRL mode (no projection)\n",
    "```\n",
    "\n",
    "### Performance Comparison\n",
    "\n",
    "| Configuration | Training Time (Phase 1 + 2) | Speedup |\n",
    "|--------------|------------------------------|---------|\n",
    "| Single GPU | ~32 hours | 1x |\n",
    "| 8 GPU DDP | ~4-5 hours | ~7-7.5x |\n",
    "| 4 GPU DDP | ~8-9 hours | ~3.8x |\n",
    "\n",
    "### Key Advantages of DDP:\n",
    "- **7-7.5x speedup** on 8 GPUs vs single GPU\n",
    "- **Balanced GPU utilization** (all GPUs ~95-98%)\n",
    "- **No GPU bottleneck** (unlike DataParallel)\n",
    "- **Efficient gradient synchronization** via NCCL All-Reduce\n",
    "\n",
    "### Configuration\n",
    "Edit `train_ddp.py` or pass command-line arguments:\n",
    "```bash\n",
    "torchrun --nproc_per_node=8 train_ddp.py \\\n",
    "    --use_projection=True \\\n",
    "    --batch_size=64 \\\n",
    "    --phase1_epochs=1 \\\n",
    "    --phase2_epochs=3 \\\n",
    "    --phase1_lr=2e-5\n",
    "```\n",
    "\n",
    "See `README_DDP.md` for complete documentation.\n",
    "\n",
    "---\n",
    "\n",
    "**Continue below for single-GPU notebook training:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load teacher model (frozen)\n",
    "logger.info(f\"Loading teacher model: {TEACHER_MODEL}\")\n",
    "teacher_model = SentenceTransformer(TEACHER_MODEL)\n",
    "teacher_model.eval()\n",
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "teacher_model = teacher_model.to(device)\n",
    "\n",
    "# Create student model with optional projection\n",
    "logger.info(f\"Loading student model: {STUDENT_MODEL}\")\n",
    "logger.info(f\"Distillation mode: {'WITH PROJECTION' if USE_PROJECTION else 'MRL-BASED (no projection)'}\")\n",
    "\n",
    "if USE_PROJECTION:\n",
    "    # Create projection layer for upsampling student embeddings\n",
    "    projection_layer = ProjectionLayer(STUDENT_DIM, PROJECTION_HIDDEN_DIM, TEACHER_DIM)\n",
    "    student_model = StudentModelWithProjection(\n",
    "        STUDENT_MODEL, \n",
    "        projection_layer=projection_layer,\n",
    "        use_projection=True\n",
    "    )\n",
    "else:\n",
    "    # No projection - use MRL slicing approach\n",
    "    student_model = StudentModelWithProjection(\n",
    "        STUDENT_MODEL,\n",
    "        projection_layer=None,\n",
    "        use_projection=False\n",
    "    )\n",
    "\n",
    "student_model = student_model.to(device)\n",
    "\n",
    "# Test models\n",
    "test_text = [\"This is a test sentence.\"]\n",
    "with torch.no_grad():\n",
    "    teacher_emb = teacher_model.encode(test_text, convert_to_tensor=True)\n",
    "    student_emb_base = student_model.encode(test_text, return_projected=False)\n",
    "    student_emb_output = student_model.encode(test_text, return_projected=True)\n",
    "    \n",
    "print(f\"\\nModel test successful:\")\n",
    "print(f\"  Teacher (full): {teacher_emb.shape}\")\n",
    "print(f\"  Student (base): {student_emb_base.shape}\")\n",
    "print(f\"  Student (output): {student_emb_output.shape}\")\n",
    "\n",
    "if USE_PROJECTION:\n",
    "    print(f\"\\n  Mode: WITH PROJECTION\")\n",
    "    print(f\"    Student {STUDENT_DIM}d -> Projection -> {TEACHER_DIM}d\")\n",
    "    print(f\"    Compared against: Teacher full {TEACHER_DIM}d\")\n",
    "else:\n",
    "    print(f\"\\n  Mode: MRL-BASED (no projection)\")\n",
    "    print(f\"    Student {STUDENT_DIM}d (direct)\")\n",
    "    print(f\"    Compared against: Teacher's first {STUDENT_DIM}d\")\n",
    "\n",
    "# Count parameters\n",
    "student_params = sum(p.numel() for p in student_model.student.parameters())\n",
    "if USE_PROJECTION:\n",
    "    projection_params = sum(p.numel() for p in student_model.projection.parameters())\n",
    "    print(f\"\\nParameter counts:\")\n",
    "    print(f\"  Student base: {student_params:,}\")\n",
    "    print(f\"  Projection: {projection_params:,}\")\n",
    "    print(f\"  Total: {student_params + projection_params:,}\")\n",
    "else:\n",
    "    print(f\"\\nParameter counts:\")\n",
    "    print(f\"  Student base: {student_params:,}\")\n",
    "    print(f\"  Projection: N/A (not used)\")\n",
    "    print(f\"  Total: {student_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Phase 1: General Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Phase 1 training\n",
    "logger.info(\"Starting Phase 1: General Distillation\")\n",
    "logger.info(f\"Config: {PHASE1_CONFIG}\")\n",
    "\n",
    "student_model = train_phase1(\n",
    "    student_model, \n",
    "    teacher_model, \n",
    "    PHASE1_CONFIG, \n",
    "    device, \n",
    "    PHASE1_CHECKPOINT\n",
    ")\n",
    "\n",
    "logger.info(f\"Phase 1 complete. Best model saved to: {PHASE1_CHECKPOINT}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Phase 1 Checkpoint and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Phase 1 checkpoint (if resuming or skipping Phase 1)\n",
    "# Uncomment if you want to load a saved Phase 1 model\n",
    "# checkpoint = torch.load(f\"{PHASE1_CHECKPOINT}.pt\")\n",
    "# student_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# logger.info(f\"Loaded Phase 1 checkpoint from: {PHASE1_CHECKPOINT}.pt\")\n",
    "\n",
    "# Quick evaluation after Phase 1\n",
    "logger.info(\"\\nEvaluating Phase 1 model...\")\n",
    "test_queries = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"How does photosynthesis work?\",\n",
    "    \"Explain quantum mechanics\"\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get teacher embeddings\n",
    "    teacher_embs = teacher_model.encode(test_queries, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    \n",
    "    # Get student embeddings\n",
    "    student_embs = student_model.encode(test_queries, normalize=True, return_projected=True)\n",
    "    \n",
    "    # Slice teacher embeddings if using MRL mode\n",
    "    if not USE_PROJECTION:\n",
    "        target_dim = student_model.get_output_dim()\n",
    "        teacher_embs_sliced = teacher_embs[:, :target_dim]\n",
    "        teacher_embs_sliced = torch.nn.functional.normalize(teacher_embs_sliced, p=2, dim=-1)\n",
    "        # Calculate cosine similarity with sliced teacher\n",
    "        cosine_sim = torch.nn.functional.cosine_similarity(teacher_embs_sliced, student_embs, dim=1)\n",
    "        print(f\"Mode: MRL-based (comparing {target_dim}d embeddings)\")\n",
    "    else:\n",
    "        # Calculate cosine similarity with full teacher\n",
    "        cosine_sim = torch.nn.functional.cosine_similarity(teacher_embs, student_embs, dim=1)\n",
    "        print(f\"Mode: With projection (comparing {TEACHER_DIM}d embeddings)\")\n",
    "    \n",
    "    print(f\"Average cosine similarity with teacher: {cosine_sim.mean().item():.4f}\")\n",
    "    print(f\"Per-query similarities: {cosine_sim.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Phase 2: Task-Specific Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Phase 2 training\n",
    "student_model = train_phase2(\n",
    "    student_model, \n",
    "    teacher_model, \n",
    "    PHASE2_CONFIG, \n",
    "    device, \n",
    "    PHASE2_CHECKPOINT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save Model to Artifacts with Proper SentenceTransformer Format\n",
    "\n",
    "from distill.save_model import save_distilled_model_to_artifacts\n",
    "\n",
    "# Save the model to artifacts/ (model name auto-generated based on mode)\n",
    "output_path = save_distilled_model_to_artifacts(\n",
    "    student_model=student_model,\n",
    "    checkpoint_path=f\"{PHASE2_CHECKPOINT}.pt\",\n",
    "    artifacts_dir=\"./artifacts\",\n",
    "    model_name=None  # Auto-generated: \"distilled-mpnet-3584d\" or \"distilled-mpnet-768d-mrl\"\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Model saved and ready to use!\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nLoad it in any script with:\")\n",
    "print(f\">>> from sentence_transformers import SentenceTransformer\")\n",
    "print(f\">>> model = SentenceTransformer('{output_path}')\")\n",
    "\n",
    "if USE_PROJECTION:\n",
    "    print(f\">>> embeddings = model.encode(['your texts here'])  # Shape: (N, {TEACHER_DIM})\")\n",
    "else:\n",
    "    print(f\">>> embeddings = model.encode(['your texts here'])  # Shape: (N, {STUDENT_DIM})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading the saved model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Use the correct model path based on mode\n",
    "if USE_PROJECTION:\n",
    "    model_path = './artifacts/distilled-mpnet-3584d'\n",
    "else:\n",
    "    model_path = './artifacts/distilled-mpnet-768d-mrl'\n",
    "\n",
    "print(f\"Loading model from: {model_path}\")\n",
    "model = SentenceTransformer(model_path)\n",
    "embeddings = model.encode(['your texts here'])\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")  # (1, 3584) or (1, 768) depending on mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best Phase 2 checkpoint\n",
    "checkpoint = torch.load(f\"{PHASE2_CHECKPOINT}.pt\")\n",
    "student_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Save the complete model\n",
    "final_model_path = os.path.join(OUTPUT_DIR, \"distilled_mpnet_final\")\n",
    "os.makedirs(final_model_path, exist_ok=True)\n",
    "\n",
    "# Save student base model\n",
    "student_model.student.save(os.path.join(final_model_path, \"student_base\"))\n",
    "\n",
    "# Save projection layer if using projection mode\n",
    "if USE_PROJECTION:\n",
    "    torch.save(\n",
    "        student_model.projection.state_dict(),\n",
    "        os.path.join(final_model_path, \"projection_layer.pt\")\n",
    "    )\n",
    "    logger.info(f\"\\nFinal model saved to: {final_model_path}\")\n",
    "    logger.info(\"  - student_base/: Base sentence-transformer model (768d)\")\n",
    "    logger.info(\"  - projection_layer.pt: Projection weights (768→3584)\")\n",
    "else:\n",
    "    logger.info(f\"\\nFinal model saved to: {final_model_path}\")\n",
    "    logger.info(\"  - student_base/: Base sentence-transformer model (768d, MRL-distilled)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Distillation Complete!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if USE_PROJECTION:\n",
    "    print(f\"\\nYou can now use the model for inference:\")\n",
    "    print(f\"  - For fast retrieval: Use 768d embeddings (student only)\")\n",
    "    print(f\"  - For hybrid system: Use 3584d embeddings (student + projection)\")\n",
    "    print(f\"  - Switch to teacher (3584d native) for hard cases\")\n",
    "else:\n",
    "    print(f\"\\nYou can now use the model for inference:\")\n",
    "    print(f\"  - The model produces 768d embeddings\")\n",
    "    print(f\"  - Trained to match teacher's first 768d (MRL)\")\n",
    "    print(f\"  - More efficient than projection-based approach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Encode queries and documents\n",
    "example_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How does neural network work?\"\n",
    "]\n",
    "\n",
    "example_docs = [\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Neural networks are computing systems inspired by biological neural networks.\"\n",
    "]\n",
    "\n",
    "# Encode with student\n",
    "with torch.no_grad():\n",
    "    query_emb = student_model.encode(example_queries, normalize=True, return_projected=True)\n",
    "    doc_emb = student_model.encode(example_docs, normalize=True, return_projected=True)\n",
    "    \n",
    "    similarities = torch.matmul(query_emb, doc_emb.T)\n",
    "    output_dim = student_model.get_output_dim()\n",
    "    mode_str = \"with projection\" if USE_PROJECTION else \"MRL-based\"\n",
    "    print(f\"\\nSimilarity scores (student model, {output_dim}d, {mode_str}):\")\n",
    "    print(similarities)\n",
    "\n",
    "# Compare with teacher\n",
    "with torch.no_grad():\n",
    "    teacher_query_emb = teacher_model.encode(example_queries, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    teacher_doc_emb = teacher_model.encode(example_docs, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    \n",
    "    # If using MRL mode, slice teacher embeddings for fair comparison\n",
    "    if not USE_PROJECTION:\n",
    "        target_dim = student_model.get_output_dim()\n",
    "        teacher_query_emb = teacher_query_emb[:, :target_dim]\n",
    "        teacher_doc_emb = teacher_doc_emb[:, :target_dim]\n",
    "        teacher_query_emb = torch.nn.functional.normalize(teacher_query_emb, p=2, dim=-1)\n",
    "        teacher_doc_emb = torch.nn.functional.normalize(teacher_doc_emb, p=2, dim=-1)\n",
    "        print(f\"\\nSimilarity scores (teacher model, first {target_dim}d, MRL slice):\")\n",
    "    else:\n",
    "        print(f\"\\nSimilarity scores (teacher model, full {TEACHER_DIM}d):\")\n",
    "    \n",
    "    teacher_similarities = torch.matmul(teacher_query_emb, teacher_doc_emb.T)\n",
    "    print(teacher_similarities)\n",
    "    \n",
    "    print(f\"\\nAverage difference: {torch.abs(similarities - teacher_similarities).mean().item():.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
