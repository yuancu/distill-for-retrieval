# Qwen as student model distillation configuration (no projection)
# This config uses direct distillation from teacher's first 1024d to student

# Query instruction for retrieval
query_instruction: "Instruct: Given a web search query, retrieve relevant passages that answer the query\nQuery: "

# Pre-computed embeddings (optional, for faster training)
# Uncomment to use pre-computed teacher embeddings
# Format: {base_dir}/{dataset_name}_{precision}/
# Example: ./cache/embedding/msmarco_fp16/
precomputed_embeddings_dir: "./cache/embedding/"

# Model configuration
model:
  teacher: "infly/inf-retriever-v1"
  student: "Qwen/Qwen3-Embedding-0.6B"
  teacher_dim: 3584
  student_dim: 1024
  projection_hidden_dim: 1536  # Not used when use_projection=false
  # Distillation mode: MRL-based (no projection)
  use_projection: false

# Phase 1: General Distillation
phase1:
  # Dataset configuration
  # Available BEIR datasets: msmarco, nfcorpus, trec-covid, nq, hotpotqa,
  #                          fiqa, arguana, scidocs, scifact, etc.
  datasets:
    - name: "msmarco"
      max_samples: 10000000

  # Training hyperparameters
  batch_size: 16
  learning_rate: 2.0e-5
  warmup_steps: 1000
  num_epochs: 3
  mse_weight: 1
  cosine_weight: 0
  max_length: 512
  gradient_accumulation_steps: 4
  precomputed_embeddings_dir: "./cache/embedding/"

# Phase 2: Task-Specific Training
phase2:
  # Dataset configuration - can specify multiple datasets
  datasets:
    - name: "msmarco"
      max_samples: 10000000

  # Training hyperparameters
  batch_size: 12
  learning_rate: 5.0e-6
  warmup_steps: 500
  num_epochs: 4
  infonce_weight: 0.8
  mse_weight: 0.2
  temperature: 0.02
  max_length: 512
  num_negatives: 7
  gradient_accumulation_steps: 8
  precomputed_embeddings_dir: "./cache/embedding/"

  # Router configuration (opt-in)
  train_router: false              # Set to true to enable router training
  router_loss_weight: 0.1          # Target weight for router loss
  router_warmup_ratio: 0.3         # Start router training after 30% of steps
  router_lr: 1.0e-4                # Learning rate for router

# Training control
training:
  skip_phase1: true
  skip_phase2: false
  save_to_artifacts: true

# Output paths
paths:
  output_dir: "./checkpoints"
  artifacts_dir: "./artifacts"
