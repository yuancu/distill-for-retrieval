# Default configuration for distillation training
# This config uses projection-based distillation (768d -> 3584d)

# Model configuration
model:
  teacher: "infly/inf-retriever-v1-pro"
  student: "sentence-transformers/all-mpnet-base-v2"
  teacher_dim: 3584
  student_dim: 768
  projection_hidden_dim: 1536

  # Distillation mode
  # - true: WITH PROJECTION (Student 768d -> Projection -> 3584d)
  #   Best for maximum quality, higher computational cost
  # - false: MRL-BASED (Student 768d -> Teacher's first 768d)
  #   Best for efficiency, lower computational cost
  use_projection: true

# Phase 1: General Distillation
# Loss: MSE (0.4) + Cosine (0.6)
# Data: Configurable BEIR datasets
phase1:
  # Dataset configuration
  # Available BEIR datasets: msmarco, nfcorpus, trec-covid, nq, hotpotqa,
  #                          fiqa, arguana, scidocs, scifact, etc.
  datasets:
    - name: "msmarco"
      max_samples: 100000
    # Uncomment to add more datasets:
    # - name: "nfcorpus"
    #   max_samples: 10000
    # - name: "hotpotqa"
    #   max_samples: 50000

  # Training hyperparameters
  batch_size: 64
  learning_rate: 2.0e-5
  warmup_steps: 1000
  num_epochs: 1
  mse_weight: 0.4
  cosine_weight: 0.6
  max_length: 512
  gradient_accumulation_steps: 4

# Phase 2: Task-Specific Training
# Loss: InfoNCE (0.8) + MSE (0.2)
# Data: Configurable datasets with hard negatives
phase2:
  # Dataset configuration - can specify multiple datasets
  # Options: msmarco, nfcorpus, trec-covid, nq, hotpotqa, fiqa, etc.
  datasets:
    - name: "msmarco"
      max_samples: 500000
    # Uncomment to add more datasets:
    # - name: "nfcorpus"
    #   max_samples: 50000

  # Training hyperparameters
  batch_size: 64
  learning_rate: 5.0e-6
  warmup_steps: 500
  num_epochs: 3
  infonce_weight: 0.8
  mse_weight: 0.2
  temperature: 0.02
  max_length: 512
  num_negatives: 7
  gradient_accumulation_steps: 8

# Training control
training:
  skip_phase1: false
  skip_phase2: false
  save_to_artifacts: false

# Output paths
# Checkpoints will be saved to: {output_dir}/{model_name}-{config_name}/
# Artifacts will be saved to: {artifacts_dir}/{model_name}-{config_name}/
paths:
  output_dir: "./checkpoints"
  artifacts_dir: "./artifacts"
