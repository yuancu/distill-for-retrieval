# MRL-based distillation configuration (no projection)
# This config uses direct distillation from teacher's first 768d to student

# Model configuration
model:
  teacher: "infly/inf-retriever-v1-pro"
  student: "sentence-transformers/all-mpnet-base-v2"
  teacher_dim: 3584
  student_dim: 768
  projection_hidden_dim: 1536  # Not used when use_projection=false

  # Distillation mode: MRL-based (no projection)
  use_projection: false

# Phase 1: General Distillation
phase1:
  # Dataset configuration
  # Available BEIR datasets: msmarco, nfcorpus, trec-covid, nq, hotpotqa,
  #                          fiqa, arguana, scidocs, scifact, etc.
  datasets:
    - name: "msmarco"
      max_samples: 1000000

  # Training hyperparameters
  batch_size: 64
  learning_rate: 2.0e-5
  warmup_steps: 1000
  num_epochs: 1
  mse_weight: 0.4
  cosine_weight: 0.6
  max_length: 512
  gradient_accumulation_steps: 4

# Phase 2: Task-Specific Training
phase2:
  # Dataset configuration - can specify multiple datasets
  datasets:
    - name: "msmarco"
      max_samples: 500000

  # Training hyperparameters
  batch_size: 16
  learning_rate: 5.0e-6
  warmup_steps: 500
  num_epochs: 3
  infonce_weight: 0.8
  mse_weight: 0.2
  temperature: 0.02
  max_length: 512
  num_negatives: 7
  gradient_accumulation_steps: 8

# Training control
training:
  skip_phase1: false
  skip_phase2: false
  save_to_artifacts: false

# Output paths
paths:
  output_dir: "./checkpoints"
  artifacts_dir: "./artifacts"
