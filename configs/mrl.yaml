# MRL-based distillation configuration (no projection)
# This config uses direct distillation from teacher's first 768d to student

# Query instruction for retrieval
query_instruction: "Instruct: Given a web search query, retrieve relevant passages that answer the query\nQuery: "

# Pre-computed embeddings (optional, for faster training)
# Uncomment to use pre-computed teacher embeddings
# Format: {base_dir}/{dataset_name}_{precision}/
# Example: ./cache/embedding/msmarco_fp16/
precomputed_embeddings_dir: "./cache/embedding/"

# Model configuration
model:
  teacher: "infly/inf-retriever-v1"
  student: "sentence-transformers/all-mpnet-base-v2"
  teacher_dim: 3584
  student_dim: 768
  projection_hidden_dim: 1536  # Not used when use_projection=false

  # Distillation mode: MRL-based (no projection)
  use_projection: false

# Phase 1: General Distillation
phase1:
  # Dataset configuration
  # Available BEIR datasets: msmarco, nfcorpus, trec-covid, nq, hotpotqa,
  #                          fiqa, arguana, scidocs, scifact, etc.
  datasets:
    - name: "msmarco"
      max_samples: 10000000

  # Training hyperparameters
  batch_size: 64
  learning_rate: 2.0e-5
  warmup_steps: 1000
  num_epochs: 2
  mse_weight: 1
  cosine_weight: 0
  max_length: 512
  gradient_accumulation_steps: 4

# Phase 2: Task-Specific Training
phase2:
  # Dataset configuration - can specify multiple datasets
  datasets:
    - name: "msmarco"
      max_samples: 500000
    - name: "nfcorpus"
      max_samples: 100000

  # Training hyperparameters
  batch_size: 16
  learning_rate: 5.0e-6
  warmup_steps: 500
  num_epochs: 3
  infonce_weight: 0.8
  mse_weight: 0.2
  temperature: 0.02
  max_length: 512
  num_negatives: 7
  gradient_accumulation_steps: 8

# Training control
training:
  skip_phase1: false
  skip_phase2: false
  save_to_artifacts: false

# Output paths
paths:
  output_dir: "./checkpoints"
  artifacts_dir: "./artifacts"
